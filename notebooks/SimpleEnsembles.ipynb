{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2260cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "MNIST_path = os.path.join(os.path.expanduser('~'), '.keras/datasets/mnist.npz')\n",
    "ROOT_DIR = '/Users/halmagyi/Documents/MachineLearning/ML_Notes/BaysianNNets/BayesianNets'\n",
    "# ROOT_DIR = '/home/ubuntu/Documents/BayesianNets'\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "os.chdir(ROOT_DIR)\n",
    "from src.mnist import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7e096b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "x_train_flat, x_test_flat, x_train_flat_bias, x_test_flat_bias, Y_train, Y_test = make_mnist_data(num_classes)\n",
    "\n",
    "num_train_samples, data_length = x_train_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96a88209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 - 4s - loss: 0.2574 - accuracy: 0.9314 - val_loss: 0.1341 - val_accuracy: 0.9670 - 4s/epoch - 2ms/step\n",
      "Epoch 2/5\n",
      "1875/1875 - 3s - loss: 0.1246 - accuracy: 0.9699 - val_loss: 0.1118 - val_accuracy: 0.9733 - 3s/epoch - 2ms/step\n",
      "Epoch 3/5\n",
      "1875/1875 - 3s - loss: 0.0938 - accuracy: 0.9794 - val_loss: 0.1013 - val_accuracy: 0.9758 - 3s/epoch - 2ms/step\n",
      "Epoch 4/5\n",
      "1875/1875 - 3s - loss: 0.0770 - accuracy: 0.9841 - val_loss: 0.0942 - val_accuracy: 0.9775 - 3s/epoch - 2ms/step\n",
      "Epoch 5/5\n",
      "1875/1875 - 3s - loss: 0.0667 - accuracy: 0.9864 - val_loss: 0.1007 - val_accuracy: 0.9758 - 3s/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "hidden_width = 300\n",
    "\n",
    "hidden_activation = 'relu'\n",
    "output_activation = 'softmax'\n",
    "\n",
    "stddev = data_length ** (-1 / 2)\n",
    "seed = 42\n",
    "kernel_initializer = initializers.RandomNormal(mean=0.0, stddev=stddev, seed=seed)\n",
    "\n",
    "use_bias = True\n",
    "dropout = False\n",
    "\n",
    "\n",
    "################\n",
    "# Regularizer\n",
    "################\n",
    "hidden_l2 = 0.00\n",
    "output_l2=0.001\n",
    "hidden_regularizer = tf.keras.regularizers.l2(hidden_l2)\n",
    "output_kernel_regularizer = tf.keras.regularizers.l2(output_l2)\n",
    "\n",
    "\n",
    "################\n",
    "# Model\n",
    "################\n",
    "\n",
    "inputs = Input(shape=(data_length,))\n",
    "\n",
    "x = Dense(hidden_width, activation=hidden_activation,\n",
    "                  use_bias=use_bias,\n",
    "                  kernel_initializer=kernel_initializer,\n",
    "                  kernel_regularizer=hidden_regularizer)(inputs)\n",
    "\n",
    "outputs = Dense(num_classes,\n",
    "                    activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer,\n",
    "                    use_bias=use_bias,\n",
    "                    kernel_regularizer=output_kernel_regularizer)(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "class EvaluateAfterBatch(Callback):\n",
    "    \"\"\"\n",
    "    A custom callback class which will evaulate data after each batch has run.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.X, self.Y = data\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "#         loss, acc = self.model.evaluate(self.X, self.Y, verbose=0)\n",
    "#         self.loss += [loss]\n",
    "#         self.acc += [acc]\n",
    "        pass\n",
    "\n",
    "callbacks = [EvaluateAfterBatch((x_test_flat, Y_test))]\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x=x_train_flat, \n",
    "          y=Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          callbacks =callbacks,\n",
    "          validation_data = (x_test_flat, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d253f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
